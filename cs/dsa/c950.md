# C950 Data Structures and Algorithms 2

## 2 - Algorithm analysis

### 2.1 Relation between data structures and algorithms

- Algorithmns to implement operations (i.e. insert, remove, etc) performed on data structure are specific to each data structure
  - Appending an item to a linked list requires a different algorithm than appending an item to an array

- Algorithms utilize data structures to store and organize data during algorithm execution

### 2.2 Constant time operations

**constant time operation**
- operation that for a given processor, always operates in the same amount of time, regardless of input values
- algorithm analysis describes runtime int terms of constant time operation rather than time since execution time can be affected by processor
- Multiple constant time operations can be collectively be considered as 1 constant time operations

**identifying constant time operations**
- Common constant time operations
  - Additions, substractions, multiplication, and division of fixed size numerical values
    - although these operations differ in execution time, they take the same amount of time (constant) regardless of the operand values   
  - variable assignment
  - comparison of 2 fixed size data values
  - array read/write at a particular index
  - A loop with constant number of iterations
    - a loop with variable number of iterations is not a constant time operation since it can take longer or shorter depending on the number of iterations
<details>
  <summary>Constant time vs non-constant time operations</summary>

  ![image](https://user-images.githubusercontent.com/14286113/173057613-c9a9b758-e400-44c3-9bac-276818a3b087.png)
</details>
  
### 2.3 Algorithm Efficiency

**algorithm efficiency**
- measured by algorithm's computational complexity (amount of resources used)
- Resources: runtime and memory usage
  - long runtimes and high memory usage means algorithm is inefficient 

**runtime complexity**
- _T(N)_ function that represents the number of constant time operations performed by algorithm on an input of size `N`
- depends on input data so runtime complexity is evaluated according to best and worst case scenarios

**Best/worst case**
- describes contents of algorithm's input data
  - input data must be variable 
- best case: scenario where the algorithm does the min possible number of operations
- worst case: scenario where the algorithm does max possible number of operations
  -  ex: searching for an element that does not exist in an array
<details>
  <summary>Ex: Linear search best and worst cases</summary>

  ![image](https://user-images.githubusercontent.com/14286113/173144424-ad068856-beda-457e-9a5a-1a1697c12143.png)

</details>

**Space complexity**
- _S(N)_ function that represents the number of fixed-size memory units used by the algorithm for an input of size N
  - includes input data + additional memory for things like loop counters and list pointers
- Auxilary space complexity: space complexity without input data
  - usually is constant since _S(N)_ is dependent on input data (_N_) but auxilary space complexity isn't
<details>
  <summary>Ex: space complexity</summary>
  
  - ![image](https://user-images.githubusercontent.com/14286113/173176867-5778f7ab-b78b-4055-95e1-c7cf1d44427c.png)
</details>

### 2.4 Growth of functions and complexity

**bounds**
- lower bound: _f(N)_ that is less than the best case _T(N)_ for all values of `N >= 1`
  - an algorithms best case runtime complexity is always a lower bound for the algorithm
- upper bound: _f(N)_ that is greater than the worst case _T(N)_ for all values of `N >= 1`
  - an algorithms worst case runtime complexity is always a upper bound for the algorithm
- bounds are used to collectively bound all runtime complexities for an algorithm, from min to max complexities
- Example:  Best-case Runtime - `T(N) = 7N + 36` & Worst-case Runtime - `T(N) = 3N^2 + 10N + 17`
  - lower bound: `f(N) = 7N`
  - upper bound: `f(N) = 30N^2`
<details>
  <summary>Upper and lower bound graph</summary>

  ![image](https://user-images.githubusercontent.com/14286113/173181760-c657c46c-a4c8-44ee-bcbf-c9613df19409.png)

</details>

**Growth rates and asymptotic notations**
- asymptotic notation: classification of runtime complexity that uses functions that indicate only the growth rate of a bounding function
- its usually a bounding function where the constants are factored out
<details>
  <summary>Notation</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173181967-ea036b62-e71b-4014-a4dd-d9d6aa1bb67a.png)
</details>

<details>
  <summary>Finding growth rates</summary>

![image](https://user-images.githubusercontent.com/14286113/173182098-f9a026d7-c764-45f8-a97a-493564bea0cd.png)

</details>


### 2.5 O Notation

**Big O notation**
- mathematical way of describing how a runtime complexity (function) behaves in relation to the input size
- different functions can have the same growth rate and therefore, same Big O notation
- growth rate is determined by the highest order term of the function (the highest exponent) while other terms are discarded/ ignored
  - constants are omitted
<details>
  <summary>Composite function Big O rules</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173187438-306da1b5-f5ee-4725-83b4-cb12c8ab18aa.png)  
</details>

<details>
  <summary>Examples</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173187615-eae478d3-c8f5-40d5-8317-5f6b10dcc037.png)
</details>

**Running Growth Rates**

- Runtime complexity for algorithms becomes important for large input sizes as the difference in computation time can vary greatly with different growth rates

<details>
  <summary>Growth Rate times</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173188022-ac4302b5-cb6f-4550-beb6-0851e1e600c7.png)
</details>

**Common Big O complexities**

<details> 
  <summary>Examples</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173188260-a341b964-4d12-40cb-9a1b-428beb530929.png)
  
  ![image](https://user-images.githubusercontent.com/14286113/173188358-b53a0214-fa39-4a66-828f-6de894bd45e8.png)
</details>

### 2.6 Algorithm analysis

**Worst-case runtime**
- ...of an algorithm is the runtime complexity for an input that results in the longest execution time
- often the main focus in algorithmn analysis

**Algorithm analysis**
- how runtime of an algorithm scales as the input size increases 
1. determine number of operations algorithm executes for a specific input size N
2/ determine the big-O notation based on the above runtime complexity function
<details>
  <summary>Algorithm Analysis Example</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173208717-0602a84c-cc69-42f3-8b08-19e4d25eee97.png)
  ![image](https://user-images.githubusercontent.com/14286113/173209059-c4c7c4b4-ee7c-4797-8655-ee2b94f0d4ce.png)
  [Simplified runtime analysis when O(1)](https://user-images.githubusercontent.com/14286113/173209134-60db40ad-693c-45fe-96cf-df44431b9b3d.png)
</details>

**How to analyze**
- _loops_
  - runtime complexity function analysis:
    - initial assignment is always executed once 
    - 2 operations per iteration for increment and comparison expressions + operations for code inside for loop block
  - big O analysis:
    - _O(N)_ for traditional for-loop that do not have nested loops, function calls, or modify the iterator in the loop
    - if for-loop is executed constant number of times instead of variable (as in i<N), then it is effectively O(1) 
- _loops where iterator is modified_
  - runtime complexity function analysis:
    - `Constant time operations * (N/x)` where `x` is the number iteration is decreased by 
      - ex: 5N/2 for 5 operations that execute when i is even number (N/2)
  - big O analysis: N/x
    - ex: N/2 for when i % 2 == 0 (even number)   
- _logN_: if iterations are decreased by some fraction each iteration, then it is considered to be logN
- _Counting constant time operations_: 
  - Ex: if/else statements, comparisons, assignments, math calculations, etc that are done 1 times (instead of N times)   
  - runtime complexity function analysis: add up the number of operations performed by each statement (?)
  - big O analysis: any statement or constant number of statements, runtime complexity is considered to be _O(1)_ or _1_
      - ex:, if there are 3 operations each loop iteration...then it can be considered to be `N` operations instead of `3N`
      - if there are 20 operations before and after a loop, then these can be collectively considered to be effectively `1` operation
  <details>
    <summary>- Constant time operations examples</summary>

    ![image](https://user-images.githubusercontent.com/14286113/173209458-90a2f800-9f29-4024-9f6f-32c41d65edab.png)
  </details>
- _Nested loops_
  - For one nested loop, big-o will be _O(N^2)_
  - For X nested loops, big-O: _O(N^x)_
  <details>
  <summary>Nested loop analysis</summary>
  
   ![image](https://user-images.githubusercontent.com/14286113/173209687-780592f4-5f93-4707-8fed-681e6d8d36a5.png)
  </details>

### 2.7 Complexity Classes

_Note: theres alot of things mentioned here that are not really explained in the chapter. I will explore these things at a later time and probably revise the notes here with more details_

**P and NP complexity classes**
- Complexity class
  -  set of problems that can be solved using a specific amount of computational resources
  -  knowing the complexity class of a problem allows a programmer to select an appropriate algorithm to solve a problem efficiently or know that the problem cannot be solved efficiently
-  P Complexity class
  - set of problems that can be solved using a deterministic machine in polynomial time
- NP complexity class
  - set of problems that can be solved using a non-determinsitic machine in polynomial time
  - problems can be solve with at most exponential runtime complexity
  <details>
    <summary>Example problems in NP</summary>
  
    ![image](https://user-images.githubusercontent.com/14286113/173236613-a4423385-8b33-4706-8309-bcced223d6ea.png)
  </details>
  <details>
    <summary>Some P & NP questions</summary>
  
    ![image](https://user-images.githubusercontent.com/14286113/173236784-aa3a083e-b31e-47ab-b1e3-9a9213df2f0d.png)
  </details>
  
**NP-Complete & NP-Hard**
- polynomial-time reducible
  - problem _A_ is polynomial-time reducible to another problem _B_ if a polynomial-time algorithm exists that maps _A_ to _B_ such that the solution to _B_ provides the solution to _A_
- NP-complete
  - set of problems in NP to which all other NP problems can be reduced in polynomial time
- NP-hard
  - set of problems that are at least as hard as NP-complete 
<details>
  <summary>NP-Complete diagram</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173250259-3240e962-cf11-4ff0-863d-84e85c9c0796.png)
</details>

<details>
  <summary>P=NP</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173250313-909c4e99-fbca-4abf-917b-e218fd56586b.png)
</details>

### 2.8 Turing Machine Introduction

_skipped_

### 2.9 Turing machine components

_skipped_

### 2.10 Python: Turing machine example

_skipped_


## 3 - Algorithms

### 3.1 Heuristics

**heuristic**
- technique that willing accepts a non-optimal or less accurate solution in order to improve execution speed
- used because solving a problem in the optimal or most accurate way may not be feasible
- sacrifices opimality or accuracy to gain speed
<details>
  <summary>Knapsack problem</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173497587-7526669b-73b6-44dd-9720-32487fad372c.png)
  ![image](https://user-images.githubusercontent.com/14286113/173498085-06fe6a08-48d7-4afa-a71e-c8b44657e2e0.png)
</details>

**self-adjusting heuristic**
- algorithm that modifies a data structure based on how that datastructure is used
- Ex: red-black trees and AVL tress use this heuristic to keep balanced trees
<details>
  <summary>Self-adjusting heuristic example</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173498690-9452be41-83cf-4ef4-b9be-4073c32d5f47.png)
</details>


### 3.2 Python solution for 0-1 Knapsack problem 

```python
from operator import attrgetter

class Item:
    def __init__(self, item_weight, item_value):
        self.weight = item_weight
        self.value = item_value
        

class Knapsack:
    def __init__(self, weight, items):
        self.max_weight = weight
        self.item_list = items


def knapsack_01(knapsack, item_list):
    # Sort the items in descending order based on value
    item_list.sort(key = attrgetter('value'), reverse = True)

    remaining = knapsack.max_weight
    for item in item_list:
        if item.weight <= remaining:
            knapsack.item_list.append(item)
            remaining = remaining - item.weight


# Main program
item_1 = Item(6, 25)
item_2 = Item(8, 42)
item_3 = Item(12, 60)
item_4 = Item(18, 95)
item_list = [item_1, item_2, item_3, item_4]
initial_knapsack_list = []

max_weight = int(input('Enter maximum weight the knapsack can hold: '))

knapsack = Knapsack(max_weight, initial_knapsack_list)
knapsack_01(knapsack, item_list)

print ('Objects in knapsack')
i = 1
sum_weight = 0
sum_value = 0

for item in knapsack.item_list:
    sum_weight += item.weight
    sum_value += item.value
    print ('%d: weight %d, value %d' % (i, item.weight,item.value))
    i += 1
print()

print('Total weight of items in knapsack: %d' % sum_weight)
print('Total value of items in knapsack: %d' % sum_value)
```
### 3.3 Greedy algorithms

**greedy algorithm**
- algorithm that when present with a list of options, chooses the option that is optimal at that point in time
- choice of option does not consider additional subsequent options
- may or may not lead to an optimal solution

<details>
  <summary>Ex: Making change equal specified amount with greedy algo</summary>

  ![image](https://user-images.githubusercontent.com/14286113/173501309-8127bb57-1534-457a-9b0a-63ffb216fd43.png)
</details>

**fractional knapsack problem**
- variation of the knapsack problem where you can take each item a fractional number of times provided the fraction is in the range of 0.0 and 1.0
- greedy solution provides optimal solution

**activity selection problem**
- problem where 1 or more activities are available, each with a start and finish time, and the goal is to build the largest possible set of activities without time conflicts
- greedy solution provides optimal solution

### 3.4 Python solutions for fractional knapsack and activity selection problems

- fractional knapsack

```python
# An individual item with a weight and value
class Item:
    def __init__(self, item_weight, item_value):
        self.weight = item_weight
        self.value = item_value
        self.fraction = 1.0

# The knapsack that contains a list of items and a maximum
# total weight
class Knapsack:
    def __init__(self, weight, items):
        self.max_weight = weight
        self.item_list = items

# A key function to be used to sort the items
def value_to_weight_ratio(item):
    return item.value / item.weight

# The Fractional Knapsack algorithm.    
def fractional_knapsack(knapsack, item_list):
    # Sort the items in descending order based on value/weight
    item_list.sort(key = value_to_weight_ratio, reverse = True)

    remaining = knapsack.max_weight
    for item in item_list:
        # Check if the full item can fit into the knapsack or
        # only a fraction
        if item.weight <= remaining:
            # The full item will fit. 
            knapsack.item_list.append(item)
            remaining = remaining - item.weight

        else:
            # Only a fractional part of the item will fit. Add that
            # fraction of the item, and then exit.
            item.fraction = remaining / item.weight
            knapsack.item_list.append(item)
            break
            
# Main program to test the fractional knapsack algorithm.
item_1 = Item(6, 25)
item_2 = Item(8, 42)
item_3 = Item(12, 60)
item_4 = Item(18, 95)
item_list = [item_1, item_2, item_3, item_4]
initial_knapsack_list = []

# Ask the user for the knapsack's maximum capacity.
max_weight = int(input('Enter maximum weight the knapsack can hold: '))


# Construct the knapsack object, then run the fractional_knapsack
# algorithm on it.
knapsack = Knapsack(max_weight, initial_knapsack_list)
fractional_knapsack(knapsack, item_list)

# Output the information about the knapsack. Show the contents
# of the knapsack, and the fraction of each item.
print ('Objects in knapsack')
i = 1
sum_weight = 0
sum_value = 0
for item in knapsack.item_list:
    sum_weight += item.weight * item.fraction
    sum_value += item.value * item.fraction
    print ('%d: %0.1f of weight %0.1f, value %0.1f' % 
          (i, item.fraction, item.weight, item.value * item.fraction))
    i += 1
print()

# Display the total weight of the items as well as the total value.
print('Total weight of items in knapsack: %d' % sum_weight)
print('Total value of items in knapsack: %d' % sum_value)
```

- activity selection problem

```python
class Activity:
    def __init__(self, name, initial_start_time, initial_finish_time):
        self.name = name
        self.start_time = initial_start_time
        self.finish_time = initial_finish_time
        
    def conflicts_with(self, other_activity):
        # No conflict exists if this activity's finish_time comes
        # at or before the other activity's start_time
        if self.finish_time <= other_activity.start_time:
            return False
            
        # No conflict exists if the other activity's finish_time
        # comes at or before this activity's start_time
        elif other_activity.finish_time <= self.start_time:
            return False
            
        # In all other cases the two activity's conflict with each
        # other
        else:
            return True

# Main program to test Activity objects
activity_1 = Activity('History museum tour', 9, 10)
activity_2 = Activity('Morning mountain hike', 9, 12)
activity_3 = Activity('Boat tour', 11, 14)

print('History museum tour conflicts with Morning mountain hike:',    
       activity_1.conflicts_with(activity_2))
print('History museum tour conflicts with Boat tour:', 
       activity_1.conflicts_with(activity_3))
print('Morning mountain hike conflicts with Boat tour:', 
       activity_2.conflicts_with(activity_3))
       
from operator import attrgetter

def activity_selection(activities):

    # Start with an empty list of selected activities
    chosen_activities = []
    
    # Sort the list of activities in increasing order of finish_time
    activities.sort(key = attrgetter("finish_time"))
    
    # Select the first activity, and add it to the chosen_activities list
    current = activities[0]
    chosen_activities.append(current)
    
    # Process all the remaining activities, in order
    for i in range(1, len(activities)):
    
        # If the next activity does not conflict with
        # the most recently selected activity, select the
        # next activity
        if not activities[i].conflicts_with(current):
            chosen_activities.append(activities[i])
            current = activities[i]
            
    # The chosen_activities list is an optimal list of
    # activities with no conflicts
    return chosen_activities

# Program to test Activity Selection greedy algorithm. The
# start_time and finish_time are represented with integers
# (ex. "20" is 20:00, or 8:00 PM).
activity_1 = Activity('Fireworks show', 20, 21)
activity_2 = Activity('Morning mountain hike', 9, 12)
activity_3 = Activity('History museum tour', 9, 10)
activity_4 = Activity('Day mountain hike', 13, 16)
activity_5 = Activity('Night movie', 19, 21)
activity_6 = Activity('Snorkeling', 15, 17)
activity_7 = Activity('Hang gliding', 14, 16)
activity_8 = Activity('Boat tour', 11, 14)

activities = [ activity_1, activity_2, activity_3, activity_4,
               activity_5, activity_6, activity_7, activity_8 ]

# Use the activity_selection() method to get a list of optimal
# activities with no conflicts.               
itinerary = activity_selection(activities)
for activity in itinerary:
    # Output the activity's information.
    print('%s - start time: %d, finish time: %d' % 
         (activity.name, activity.start_time, activity.finish_time))
```

### 3.5 Dynamic Programming

**dynamic programming**
- problem solving technique that splits a problem into smaller subproblems, computes and stores solutions to subproblems in memory, and then uses the stored solutions to solve the larger problem
- Ex: fibonacci numbers can be computed with iterative approach that stores 2 previous terms, instead of making recrusive calls that recompute the same term many times over
- avoids redundant computations by storing and resuing previous computation results

**longest common substring algorithm**
- takes 2 strings as input and determines the longest substring that exists in both strings
- uses dynamic programming
- Runtime complexity: O(N X M)
  - space complexity can be reduced to O(N) by storing only the previously computed row and larget matrix entry's location and value 
- ex: finding common substrings in DNA strings

## 4 Trees

### 4.1 Binary Trees

**binary tree**
- where each node has up to children, known as left and right child
- leaf: a tree node with no children
- internal node: a node with at least one child
- parent: a node with a child is set to be that child's parent
- root: the one tree node with no parent; AKA topmost node
- edge: link from a node to a child
- depth: depth of a node is the number of edges on the path from the root to the node; root node has depth 0
- level: all nodes with the same depth
- height: trees height is the largest depth of any node; tree with just one node has height 0
<details>
  <summary>Basic Binary Tree Description</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173886521-3fa5fb61-94c1-4afe-8ac0-2d9d4b3f95ee.png)
  ![image](https://user-images.githubusercontent.com/14286113/173887088-62d8d8b7-8a13-466c-b7e1-16df0ac374f9.png)
</details>

**Types of Binary Trees**
- full binary tree: full if every node contains 0 or 2 children
- complete binary tree: complete if all levels expect possible the last level, are completely full and all nodes in the last level are as far left as possible
- perfect binary tree: perfect if all internal nodes have 2 children and all leaf nodes are at the same level
<details>
  <summary>Special types of binary trees</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173887911-99052b97-3e94-470e-bd68-db86f52b7618.png)
</details>

## 4.2 Application of Trees

<details>
  <summary>Trees can be used to represent hierarchical data</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173902539-2993febd-a0dd-4474-869e-8ba064112513.png)
</details>

**binary space partitioning (BSP)**
- technique of repeatedly separating a region of space into 2 parts and catologing objects contained within the regions
- bsp tree: binary tree used to store information for BSP
  - each node in a BSP tree contains information about a region of space and which objects are contained in the region
<details>
  <summary>BSP tree being used to quickly determine which objects do not need to be rendered</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173903399-acd4de82-986b-4ed8-a66b-2e01c720a424.png)
</details>
 
### 4.3 Binary Search Tree

**Binary Search Tree (BST)**
- allows for duplicate values
- has an ordering property where parent node's
  - left child key will be less than the parent key
  - right child key will be greater than the parent key
- enables fast searching for an item
<details>
  <summary>Example of BST</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173909890-4de720f3-57b0-45c6-bcc3-770f5a28a12e.png)
  ![image](https://user-images.githubusercontent.com/14286113/173915432-f729aec3-5814-49da-9a8a-d8b6616e4e8b.png)
</details>

**BST Traversal**
- ordering among nodes is smallest (bottom left) to largest (bottom right)
  - node is traversed along the successor/ predecessor chain 
- successor (next)
  - node's successor is the node that comes after in the BST ordering
- Predecessor (previous)
  - node's predecessor is the node that comes before in the BST ordering
- Traversal rules
  - for any given node, if that node has a right subtree, then the node's successor is that right subtree's leftmost child
    - starting from the right subtree's root follow left children until reaching a node with no left child (could be the root itself)
    - if node does not have a right subtree, then the node's sucessor is the first ancestor having that node in a left subtree 
<details>
  <summary>BST Traversal/ ordering example</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173918032-16406693-5b94-46a3-8979-52f0ae5d98b6.png)
  ![image](https://user-images.githubusercontent.com/14286113/173919031-1ac3929b-be08-48cf-a10a-a328b55907f0.png)
</details>

**Searching**
- searching means to find a node with a desired key if such a node exists
- BST yields faster searches than a list (faster = less comparisions)
- Search begins at the root node and then the left and right subtrees are visited according to the key value

<details>
  <summary>BST Searching Example</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/173915965-120736e4-5066-40c7-9d96-c4a8b5c1c2cc.png)
  ![image](https://user-images.githubusercontent.com/14286113/173917218-7b16378d-b6dd-4f99-b850-a885041361fc.png)
</details>

**BST search algorithm:**
- Given a key, returns the first node found matching the key or null if a matching node is not found
- Check current node (trees root)
  - if key matched, return that node
  - else reassign current node with left subtree if key is less or right if key is greater, and repeat
  - else if child is null, return null

**Best case BST search runtime**
- "All but last level full binary tree" height = log2N, where N = node
- Worst case: H+1 comparisions, O(H) 
- Best-case: O(LogN)

### 4.5 BST Insert Algorithm

**insert algorithm description**
- new nodes are inserted in a way that doesn't violate the BST ordering property (left/smallest to right/largest)
- Compare new node with current node (initially the root)
  - Insert as left child of current node if:
    -  new node's key is less than current node's key
    -  current node's left child is null
  - Insert as right child of current node if:
    - new node's key is greater than the current node
    - current node's right child is null
  - reassign current node to child if:
    - left or right child is not null

**insert time complexity**
- best case: O(LogN)
- worst case: O(N)
- Space complexity: O(1)

### 4.6 BST Remove Algorithm

**remove algorithm description**
- removes first-founding matching node, restructuring the tree to preserve the BST ordering property 
> The algorithm first searches for a matching node just like the search algorithm. If found (call this node X), the algorithm performs one of the following sub-algorithms:

> Remove a leaf node: If X has a parent (so X is not the root), the parent's left or right child (whichever points to X) is assigned with null. Else, if X was the root, the root pointer is assigned with null, and the BST is now empty.
> Remove an internal node with single child: If X has a parent (so X is not the root), the parent's left or right child (whichever points to X) is assigned with X's single child. Else, if X was the root, the root pointer is assigned with X's single child.

> Remove an internal node with two children: This case is the hardest. First, the algorithm locates X's successor (the leftmost child of X's right subtree), and copies the successor to X. Then, the algorithm recursively removes the successor from the right subtree.

**time complexity**
- best case: O(LogN)
- worst case: O(N)

### 4.7 BST inorder traversal

**tree traversal algorithm**
- visits all nodes in the tree once and performs an operation on each node

**inorder traversal**
- visits all nodes in a BST from smallest to largest, recursively
- useful for printing tree's nodes in a sorted order

## 5 Balanced Trees

### 5.1 AVL: A balanced tree

**AVL tree**
- BST with a height balance property and specific operations to rebalance the tree when a node is inserted or removed

**Height balancing**
- a BST is height balanced if for any node, the heights of the node's left and right subtrees differ by only 0 or 1

**Balance factor**
- a node's balance factor is the left subtree height minus the right subtree height, which is 1,0, or -1 in an AVL tree
- NOTE: tree with just one one node has height 0
<details>
  <summary>Balance factor of trees example</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/178663907-1241839c-1c91-465b-98bc-eb7993454bd0.png)
  ![image](https://user-images.githubusercontent.com/14286113/178664350-83141d01-8670-4aa7-a092-9c2cde428a8e.png)
</details>

### 5.2 AVL rotations

**rotation**
- local rearrangement of a BST that maintains the BST ordering property while rebalancing the tree
- allows tree to maintain height balance
<details>
  <summary>Simple AVL rotation</summary>
  
  ![image](https://user-images.githubusercontent.com/14286113/178665147-9cffc4e5-5174-4c6c-9733-e50c44bc2e85.png)
</details>

### 5.3 AVL Insertions

_skipped_

### 5.4 AVL removals

_skipped_

### 5.5 AVL Python implementation

_skipped_





